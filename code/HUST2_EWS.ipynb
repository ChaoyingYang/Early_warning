{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85501a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def bacon_watts_method(time, capacity):\n",
    "    \"\"\"\n",
    "    Detect the turning point (knee point) in the capacity curve using the Bacon-Watts method.\n",
    "    \n",
    "    Parameters:\n",
    "    - time: Original time data (e.g., cycle number)\n",
    "    - capacity: Original capacity data\n",
    "\n",
    "    Returns:\n",
    "    - best_split_index: Index of the knee point in the data\n",
    "    - jump_point_time: Original time value corresponding to the knee point\n",
    "    - jump_point_capacity: Original capacity value corresponding to the knee point\n",
    "    \"\"\"\n",
    "    n = len(time)\n",
    "    best_split_index = 0\n",
    "    min_total_error = float('inf')\n",
    "\n",
    "    # Traverse all possible split points\n",
    "    for split in range(1, n - 1):\n",
    "        # Split the data into left and right segments\n",
    "        time_left, capacity_left = time[:split], capacity[:split]\n",
    "        time_right, capacity_right = time[split:], capacity[split:]\n",
    "        \n",
    "        # Fit linear models to the left and right segments\n",
    "        model_left = LinearRegression().fit(time_left.reshape(-1, 1), capacity_left)\n",
    "        model_right = LinearRegression().fit(time_right.reshape(-1, 1), capacity_right)\n",
    "        \n",
    "        # Calculate prediction errors for both segments\n",
    "        predicted_left = model_left.predict(time_left.reshape(-1, 1))\n",
    "        predicted_right = model_right.predict(time_right.reshape(-1, 1))\n",
    "        error_left = np.sum((capacity_left - predicted_left) ** 2)\n",
    "        error_right = np.sum((capacity_right - predicted_right) ** 2)\n",
    "        \n",
    "        # Compute total error\n",
    "        total_error = error_left + error_right\n",
    "\n",
    "        # Update best split point if current split yields a smaller error\n",
    "        if total_error < min_total_error:\n",
    "            min_total_error = total_error\n",
    "            best_split_index = split\n",
    "\n",
    "    # Retrieve the original values at the knee point\n",
    "    jump_point_time = time[best_split_index]\n",
    "    jump_point_capacity = capacity[best_split_index]\n",
    "\n",
    "    return best_split_index, jump_point_time, jump_point_capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43517876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1 length is 100 v_max: 0.9442 q_max: 0.9074 dv_max: 0.0228 dq_max: 0.0000\n",
      "1-2 length is 100 v_max: 0.9492 q_max: 0.9439 dv_max: 0.0196 dq_max: 0.0000\n",
      "1-3 length is 100 v_max: 0.9208 q_max: 0.9233 dv_max: 0.0000 dq_max: 0.0053\n",
      "1-4 length is 100 v_max: 0.9183 q_max: 0.9234 dv_max: 0.0104 dq_max: 0.0052\n",
      "1-5 length is 100 v_max: 0.9271 q_max: 0.9128 dv_max: 0.0013 dq_max: 0.0079\n",
      "1-6 length is 100 v_max: 0.9596 q_max: 0.9022 dv_max: 0.0283 dq_max: 0.0000\n",
      "1-7 length is 100 v_max: 0.9429 q_max: 0.9232 dv_max: 0.0351 dq_max: 0.0000\n",
      "1-8 length is 100 v_max: 0.9429 q_max: 0.9363 dv_max: 0.0281 dq_max: 0.0000\n",
      "2-2 length is 100 v_max: 0.9429 q_max: 0.9753 dv_max: 0.0023 dq_max: 0.0024\n",
      "2-3 length is 100 v_max: 0.9092 q_max: 0.9285 dv_max: 0.0337 dq_max: 0.0079\n",
      "2-4 length is 100 v_max: 0.9400 q_max: 0.9232 dv_max: 0.0230 dq_max: 0.0000\n",
      "2-5 length is 100 v_max: 0.9246 q_max: 0.9149 dv_max: 0.0142 dq_max: 0.0079\n",
      "2-6 length is 100 v_max: 0.9092 q_max: 0.9439 dv_max: 0.0167 dq_max: 0.0078\n",
      "2-7 length is 100 v_max: 0.9154 q_max: 0.9570 dv_max: 0.0258 dq_max: 0.0078\n",
      "2-8 length is 100 v_max: 0.9337 q_max: 0.9150 dv_max: 0.0373 dq_max: 0.0022\n",
      "3-1 length is 100 v_max: 0.9042 q_max: 0.9543 dv_max: 0.0233 dq_max: 0.0026\n",
      "3-2 length is 100 v_max: 0.9092 q_max: 0.9596 dv_max: 0.0308 dq_max: 0.0052\n",
      "3-3 length is 100 v_max: 0.9054 q_max: 0.9412 dv_max: 0.0067 dq_max: 0.0026\n",
      "3-4 length is 100 v_max: 0.9350 q_max: 0.9123 dv_max: 0.0292 dq_max: 0.0023\n",
      "3-5 length is 100 v_max: 0.9454 q_max: 0.9913 dv_max: 0.0221 dq_max: 0.0000\n",
      "3-6 length is 100 v_max: 0.9104 q_max: 0.9493 dv_max: 0.0321 dq_max: 0.0052\n",
      "3-7 length is 100 v_max: 0.9350 q_max: 0.9094 dv_max: 0.0347 dq_max: 0.0025\n",
      "3-8 length is 100 v_max: 0.9054 q_max: 0.9807 dv_max: 0.0313 dq_max: 0.0026\n",
      "4-1 length is 100 v_max: 0.9221 q_max: 0.9305 dv_max: 0.0412 dq_max: 0.0026\n",
      "4-2 length is 100 v_max: 0.9042 q_max: 0.9521 dv_max: 0.0246 dq_max: 0.0053\n",
      "4-3 length is 100 v_max: 0.9363 q_max: 0.9126 dv_max: 0.0372 dq_max: 0.0000\n",
      "4-4 length is 100 v_max: 0.9337 q_max: 0.9070 dv_max: 0.0370 dq_max: 0.0000\n",
      "4-5 length is 100 v_max: 0.9388 q_max: 0.9423 dv_max: 0.0269 dq_max: 0.0000\n",
      "4-6 length is 100 v_max: 0.9413 q_max: 0.9021 dv_max: 0.0303 dq_max: 0.0000\n",
      "4-7 length is 100 v_max: 0.9467 q_max: 0.9857 dv_max: 0.0132 dq_max: 0.0024\n",
      "4-8 length is 100 v_max: 0.9104 q_max: 0.9522 dv_max: 0.0129 dq_max: 0.0053\n",
      "5-1 length is 100 v_max: 0.9504 q_max: 0.9472 dv_max: 0.0039 dq_max: 0.0001\n",
      "5-2 length is 100 v_max: 0.9129 q_max: 0.9341 dv_max: 0.0233 dq_max: 0.0079\n",
      "5-3 length is 100 v_max: 0.9517 q_max: 0.9419 dv_max: 0.0154 dq_max: 0.0000\n",
      "5-4 length is 100 v_max: 0.9442 q_max: 0.9161 dv_max: 0.0250 dq_max: 0.0002\n",
      "5-5 length is 100 v_max: 0.9196 q_max: 0.9338 dv_max: 0.0196 dq_max: 0.0052\n",
      "5-6 length is 100 v_max: 0.9504 q_max: 0.9466 dv_max: 0.0063 dq_max: 0.0000\n",
      "5-7 length is 100 v_max: 0.9413 q_max: 0.9155 dv_max: 0.0328 dq_max: 0.0000\n",
      "6-1 length is 100 v_max: 0.9429 q_max: 0.9054 dv_max: 0.0196 dq_max: 0.0000\n",
      "6-2 length is 100 v_max: 0.9154 q_max: 0.9547 dv_max: 0.0242 dq_max: 0.0077\n",
      "6-3 length is 100 v_max: 0.9092 q_max: 0.9470 dv_max: 0.0117 dq_max: 0.0026\n",
      "6-4 length is 100 v_max: 0.9337 q_max: 0.9154 dv_max: 0.0270 dq_max: 0.0000\n",
      "6-5 length is 100 v_max: 0.9442 q_max: 0.9262 dv_max: 0.0255 dq_max: 0.0000\n",
      "6-6 length is 100 v_max: 0.9312 q_max: 0.9282 dv_max: 0.0183 dq_max: 0.0077\n",
      "6-8 length is 100 v_max: 0.9504 q_max: 0.9492 dv_max: 0.0233 dq_max: 0.0000\n",
      "7-1 length is 100 v_max: 0.9221 q_max: 0.9312 dv_max: 0.0307 dq_max: 0.0000\n",
      "7-2 length is 100 v_max: 0.8846 q_max: 0.9703 dv_max: 0.0321 dq_max: 0.0026\n",
      "7-3 length is 100 v_max: 0.9283 q_max: 0.9103 dv_max: 0.0463 dq_max: 0.0002\n",
      "7-4 length is 100 v_max: 0.9312 q_max: 0.9126 dv_max: 0.0380 dq_max: 0.0000\n",
      "7-5 length is 100 v_max: 0.8937 q_max: 0.9571 dv_max: 0.0104 dq_max: 0.0001\n",
      "7-6 length is 100 v_max: 0.9283 q_max: 0.9173 dv_max: 0.0431 dq_max: 0.0023\n",
      "7-7 length is 100 v_max: 0.9325 q_max: 0.9180 dv_max: 0.0293 dq_max: 0.0000\n",
      "7-8 length is 100 v_max: 0.9000 q_max: 0.9548 dv_max: 0.0400 dq_max: 0.0053\n",
      "8-1 length is 100 v_max: 0.9363 q_max: 0.9158 dv_max: 0.0435 dq_max: 0.0001\n",
      "8-2 length is 100 v_max: 0.9454 q_max: 0.9442 dv_max: 0.0183 dq_max: 0.0000\n",
      "8-3 length is 100 v_max: 0.9183 q_max: 0.9530 dv_max: 0.0013 dq_max: 0.0026\n",
      "8-4 length is 100 v_max: 0.9363 q_max: 0.9233 dv_max: 0.0369 dq_max: 0.0000\n",
      "8-5 length is 100 v_max: 0.9400 q_max: 0.9100 dv_max: 0.0335 dq_max: 0.0001\n",
      "8-6 length is 100 v_max: 0.9429 q_max: 0.9814 dv_max: 0.0162 dq_max: 0.0000\n",
      "8-7 length is 100 v_max: 0.9325 q_max: 0.9286 dv_max: 0.0415 dq_max: 0.0000\n",
      "8-8 length is 100 v_max: 0.9117 q_max: 0.9603 dv_max: 0.0075 dq_max: 0.0026\n",
      "9-1 length is 100 v_max: 0.8975 q_max: 0.9813 dv_max: 0.0035 dq_max: 0.0000\n",
      "9-2 length is 100 v_max: 0.8987 q_max: 0.9698 dv_max: 0.0221 dq_max: 0.0026\n",
      "9-3 length is 100 v_max: 0.9000 q_max: 0.9707 dv_max: 0.0437 dq_max: 0.0001\n",
      "9-4 length is 100 v_max: 0.9000 q_max: 0.9761 dv_max: 0.0450 dq_max: 0.0027\n",
      "9-5 length is 100 v_max: 0.9025 q_max: 0.9578 dv_max: 0.0283 dq_max: 0.0002\n",
      "9-6 length is 100 v_max: 0.9067 q_max: 0.9701 dv_max: 0.0171 dq_max: 0.0000\n",
      "9-7 length is 100 v_max: 0.8987 q_max: 0.9627 dv_max: 0.0192 dq_max: 0.0026\n",
      "9-8 length is 100 v_max: 0.9142 q_max: 0.9468 dv_max: 0.0117 dq_max: 0.0053\n",
      "10-1 length is 100 v_max: 0.9271 q_max: 0.9206 dv_max: 0.0410 dq_max: 0.0000\n",
      "10-2 length is 100 v_max: 0.8950 q_max: 0.9658 dv_max: 0.0296 dq_max: 0.0027\n",
      "10-3 length is 100 v_max: 0.9283 q_max: 0.9315 dv_max: 0.0374 dq_max: 0.0000\n",
      "10-4 length is 100 v_max: 0.9054 q_max: 0.9524 dv_max: 0.0196 dq_max: 0.0026\n",
      "10-5 length is 100 v_max: 0.9067 q_max: 0.9545 dv_max: 0.0129 dq_max: 0.0026\n",
      "10-6 length is 100 v_max: 0.9400 q_max: 0.9966 dv_max: 0.0204 dq_max: 0.0000\n",
      "10-7 length is 100 v_max: 0.9117 q_max: 0.9525 dv_max: 0.0363 dq_max: 0.0053\n",
      "10-8 length is 100 v_max: 0.9363 q_max: 0.9290 dv_max: 0.0348 dq_max: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tool import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score,mean_squared_error,mean_absolute_error, r2_score\n",
    "from net import CRNN\n",
    "from common1 import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Conv1d\n",
    "from kneed import KneeLocator\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import scipy\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_cyc = 30\n",
    "in_stride = 3\n",
    "fea_num = 100\n",
    "\n",
    "v_low = 3.36\n",
    "v_upp = 3.60\n",
    "q_low = 610\n",
    "q_upp = 1190\n",
    "rul_factor = 3000\n",
    "cap_factor = 1190\n",
    "\n",
    "pkl_list = os.listdir('./data/our_data/')\n",
    "pkl_list = sorted(pkl_list, key=lambda x:int(x.split('-')[0])*10 + int(x[-5]))\n",
    "\n",
    "tot_name = copy.deepcopy(pkl_list)\n",
    "\n",
    "train_name = []\n",
    "for name in pkl_list:\n",
    "    train_name.append(name[:-4])\n",
    "\n",
    "\n",
    "all_loader = dict()\n",
    "\n",
    "for name in train_name:\n",
    "    tmp_x, tmp_cap1, tmp_rul2, tmp_cap3 = get_xl(name, fea_num, v_low, v_upp, q_low, q_upp, rul_factor, cap_factor)\n",
    "    all_loader.update({name:{'data_x':tmp_x,'data_cap1':tmp_cap1,'data_rul2':tmp_rul2.reshape(-1,1),'data_cap3':tmp_cap3}})\n",
    "\n",
    "new_valid = ['4-3', '5-7', '3-3', '2-3', '9-3', '10-5', '3-2', '3-7']\n",
    "new_train = ['9-1', '2-2', '4-7','9-7', '1-8','4-6','2-7','8-4', '7-2','10-3', '2-4', '7-4', '3-4',\n",
    "            '5-4', '8-7','7-7', '4-4','1-3', '7-1','5-2', '6-4', '9-8','9-5','6-3','10-8','1-6','3-5',\n",
    "              '2-6', '3-8', '3-6', '4-8', '7-8','5-1', '2-8', '8-2','1-5','7-3', '10-2','5-5', '9-2','5-6', '1-7', \n",
    "              '8-3', '4-1','4-2','1-4','6-5', ]\n",
    "new_test  = ['9-6','4-5','1-2', '10-7','1-1', '6-1','6-6', '9-4','10-4','8-5', '5-3','10-6',\n",
    "            '2-5','6-2','3-1','8-8', '8-1','8-6','7-6','6-8','7-5','10-1'] \n",
    "\n",
    "train_x, train_cap1, train_rul2, train_cap3, train_eol, train_tp = [], [], [], [], [], []\n",
    "for name in new_train + new_valid:\n",
    "    tmp_x, tmp_cap1, tmp_rul2, tmp_cap3 = all_loader[name]['data_x'],all_loader[name]['data_cap1'],all_loader[name]['data_rul2'],all_loader[name]['data_cap3']\n",
    "    train_eol.append(min(np.where(np.array(tmp_cap3*cap_factor/(max(tmp_cap3)*cap_factor)) < 0.8)[0]))\n",
    "    train_x.append(tmp_x)\n",
    "    train_rul2.append(tmp_rul2)\n",
    "    train_cap3.append(np.array(tmp_cap3)/max(tmp_cap3)) \n",
    "    \n",
    "    capacity = np.array(train_cap3[-1]*cap_factor) \n",
    "    time = np.array([x for x in range(len(capacity))])\n",
    "\n",
    "\n",
    "    best_split_index, jump_point_time, jump_point_capacity = bacon_watts_method(time, capacity)\n",
    "    train_tp.append(jump_point_time)\n",
    "\n",
    "\n",
    "valid_x, valid_cap1, valid_rul2, valid_cap3, valid_eol, valid_tp = [], [], [], [], [], []\n",
    "for name in new_test:\n",
    "    tmp_x, tmp_cap1, tmp_rul2, tmp_cap3 = all_loader[name]['data_x'],all_loader[name]['data_cap1'],all_loader[name]['data_rul2'],all_loader[name]['data_cap3']\n",
    "    valid_eol.append(min(np.where(np.array(tmp_cap3*cap_factor/(max(tmp_cap3)*cap_factor)) < 0.8)[0]))\n",
    "    valid_x.append(tmp_x)\n",
    "    valid_rul2.append(tmp_rul2)\n",
    "    valid_cap3.append(np.array(tmp_cap3)/max(tmp_cap3)) \n",
    " \n",
    "    capacity = np.array(valid_cap3[-1]*cap_factor)  \n",
    "    time = np.array([x for x in range(len(capacity))])\n",
    "\n",
    "\n",
    "    best_split_index, jump_point_time, jump_point_capacity = bacon_watts_method(time, capacity)\n",
    "    valid_tp.append(jump_point_time)\n",
    "    \n",
    "tot_eol = train_eol + valid_eol\n",
    "tot_cap = train_cap3 + valid_cap3\n",
    "tot_tp = train_tp + valid_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def func(x, D1, D2):\n",
    "    return D1 * np.sqrt(x) + D2 * np.power(x, 1.5)\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.where(y_true == 0, 1e-6, y_true)  # 避免除0\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# 结果容器\n",
    "tot_r2 = []\n",
    "tot_rmse = []\n",
    "tot_mape = []\n",
    "\n",
    "tot_jl = []\n",
    "tot_Tcap = []\n",
    "\n",
    "for num in range(len(tot_cap)):\n",
    "    # 生命周期区间\n",
    "    Cycle = np.arange(tot_eol[num])\n",
    "    \n",
    "    # 平滑容量 + 损失计算\n",
    "    cap_tmp = savgol_filter(tot_cap[num][:tot_eol[num]], 20, 4)\n",
    "    Qloss = tot_cap[num][0] - cap_tmp\n",
    "\n",
    "    # 拟合整个区间\n",
    "    popt, _ = curve_fit(func, Cycle, Qloss)\n",
    "    \n",
    "    # 预测全周期\n",
    "    Qloss_pred = func(Cycle, *popt)\n",
    "\n",
    "    # 存储拟合曲线和真实曲线\n",
    "    tot_jl.append(Qloss_pred)\n",
    "    tot_Tcap.append(Qloss)\n",
    "\n",
    "    # 全周期评估\n",
    "    tot_r2.append(r2_score(Qloss, Qloss_pred))\n",
    "    tot_rmse.append(rmse(Qloss, Qloss_pred))\n",
    "    tot_mape.append(mape(Qloss, Qloss_pred))\n",
    "\n",
    "# 平均指标输出\n",
    "print(\"平均 R²:\", np.mean(tot_r2))\n",
    "print(\"平均 RMSE:\", np.mean(tot_rmse))\n",
    "print(\"平均 MAPE (%):\", np.mean(tot_mape))\n",
    "\n",
    "# 可视化 R²\n",
    "x = np.arange(1, len(tot_cap) + 1)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, tot_r2, color='blue', label='R²')\n",
    "plt.xlabel('Sample', fontsize=14)\n",
    "plt.ylabel('R² Score', fontsize=14)\n",
    "plt.title('R² of full-length fit across samples', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e55067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection success rate: 0.9362\n",
      "Precision: 0.8182\n",
      "Mean prediction advance time (cycles): 306.7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.signal\n",
    "\n",
    "# Define nonlinear model: y = A * sqrt(x) + B * x^1.5\n",
    "def model(A, B, x):\n",
    "    return A * np.sqrt(x) + B * np.power(x, 1.5)\n",
    "\n",
    "\n",
    "# Recursive Least Squares with regularization and soft constraints\n",
    "class RecursiveLeastSquares:\n",
    "    def __init__(self, lambda_factor=0.98, delta=1e3, regularization_factor=0.02):\n",
    "        # Initialize parameters\n",
    "        self.lambda_factor = lambda_factor  # Forgetting factor\n",
    "        self.regularization_factor = regularization_factor  # Regularization strength\n",
    "        self.theta = np.array([1e-4, 0], dtype=np.float64)  # Initial parameters [A, B]\n",
    "        self.P = np.eye(2, dtype=np.float64) * delta  # Initial covariance matrix\n",
    "    \n",
    "    def update(self, x, y):\n",
    "        # Feature vector φ_t = [sqrt(x), x^1.5]\n",
    "        phi_t = np.array([np.sqrt(x), np.power(x, 1.5)], dtype=np.float64)\n",
    "        \n",
    "        # Gain matrix K_t\n",
    "        P_phi = self.P @ phi_t\n",
    "        gain = P_phi / (self.lambda_factor + phi_t.T @ P_phi)\n",
    "        \n",
    "        # Update parameters θ_t\n",
    "        self.theta += gain * (y - phi_t.T @ self.theta)\n",
    "        \n",
    "        # Apply soft constraint to prevent A from becoming negative\n",
    "        if self.theta[0] < 0:\n",
    "            self.theta[0] += self.regularization_factor * abs(self.theta[0]) * 4\n",
    "            if self.theta[0] < 0:\n",
    "                self.theta[0] = 0\n",
    "        \n",
    "        # Update covariance matrix P_t\n",
    "        self.P = (self.P - np.outer(gain, phi_t.T @ self.P)) / self.lambda_factor\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.theta  # Return [A, B]\n",
    "\n",
    "\n",
    "# Batch initialization\n",
    "def batch_initialize(x_data, y_data, rls, batch_size):\n",
    "    for i in range(batch_size):\n",
    "        rls.update(x_data[i], y_data[i])\n",
    "\n",
    "\n",
    "# Global experiment parameters\n",
    "window_size = 30  # Rolling window size\n",
    "increase_length = 29\n",
    "threshold = 2\n",
    "\n",
    "# Split the temporary dataset into validation and test sets\n",
    "valid_tot_cap, test_tot_cap, valid_tot_eol, test_tot_eol, valid_tot_tp, test_tot_tp = train_test_split(\n",
    "    tot_cap, tot_eol, tot_tp, test_size=0.6, random_state=0\n",
    ")\n",
    "\n",
    "tot_cap_data = test_tot_cap\n",
    "tot_tp_data = test_tot_tp\n",
    "tot_eol_data = test_tot_eol\n",
    "threshold = threshold * 1e-6\n",
    "tot_tp_pre = []\n",
    "\n",
    "# Run experiment on each sample\n",
    "for num in range(len(tot_cap_data)):\n",
    "    Cycle = [x for x in range(5000)]\n",
    "    Qloss = max(tot_cap_data[num]) - np.array(tot_cap_data[num])\n",
    "    y_data = Qloss[:tot_eol_data[num]]\n",
    "    y_data = scipy.signal.savgol_filter(y_data, 50, 4)\n",
    "    x_data = np.array(Cycle[:tot_eol_data[num]]) / 10000\n",
    "\n",
    "    # Initialize RLS\n",
    "    rls = RecursiveLeastSquares(lambda_factor=0.99, delta=1e3)\n",
    "\n",
    "    # Batch initialization\n",
    "    batch_initialize(x_data, y_data, rls, batch_size=10)\n",
    "\n",
    "    # Store parameter history\n",
    "    A_history = []\n",
    "    B_history = []\n",
    "    r2_history = []\n",
    "    fitted_capacity_history = []\n",
    "\n",
    "    # Recursive updates\n",
    "    for t, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "        rls.update(x, y)\n",
    "        A, B = rls.get_params()\n",
    "\n",
    "        A_history.append(A)\n",
    "        B_history.append(B)\n",
    "\n",
    "        y_fit_current = model(A, B, x_data[:t + 1])\n",
    "        r2_value = r2_score(y_data[:t + 1], y_fit_current)\n",
    "        r2_history.append(r2_value)\n",
    "\n",
    "        fitted_capacity_history.append(y_fit_current[-1])\n",
    "\n",
    "    t = np.arange(0, len(A_history))\n",
    "    series = pd.Series(A_history)\n",
    "\n",
    "    # Compute rolling variance\n",
    "    variance_signal = series.rolling(window=window_size).var()\n",
    "    moving_avg = np.convolve(variance_signal, np.ones(window_size) / window_size, mode='valid')\n",
    "    padding = np.full(window_size - 1, np.nan)\n",
    "    variance_signal = np.concatenate((padding, moving_avg))\n",
    "\n",
    "    increasing_start_idx = None\n",
    "    increase_count = 0\n",
    "\n",
    "    # Detect start of consistent increase\n",
    "    for idx in range(100, len(variance_signal)):\n",
    "        if variance_signal[idx] > variance_signal[idx - 1] and variance_signal[idx] > threshold:\n",
    "            increase_count += 1\n",
    "            if increase_count >= increase_length:\n",
    "                increasing_start_idx = idx - increase_length + 1\n",
    "                break\n",
    "        else:\n",
    "            increase_count = 0\n",
    "\n",
    "    # Ensure no out-of-bounds\n",
    "    if increasing_start_idx is not None:\n",
    "        increasing_start_time = increasing_start_idx\n",
    "    else:\n",
    "        increasing_start_time = len(A_history)\n",
    "\n",
    "    tot_tp_pre.append(increasing_start_time)\n",
    "\n",
    "\n",
    "# Evaluate predictions\n",
    "prd_er = np.array(tot_tp_data) - np.array(tot_tp_pre)\n",
    "pos_prd_er = []\n",
    "pos_tp = []\n",
    "pos_pr = []\n",
    "true_positive = []\n",
    "all_prediction_advance_times = []\n",
    "\n",
    "for i in range(len(prd_er)):\n",
    "    if tot_tp_pre[i] is not None:\n",
    "        time_diff = prd_er[i]\n",
    "        if time_diff >= 0:\n",
    "            pos_pr.append(tot_tp_pre[i])\n",
    "            pos_prd_er.append(time_diff)\n",
    "\n",
    "        # Consider as true positive if within 20% of actual failure time\n",
    "        if 0 < time_diff <= tot_eol_data[i] * 0.2:\n",
    "            true_positive.append(tot_tp_pre[i])\n",
    "\n",
    "# Calculate metrics\n",
    "tp = len(true_positive)\n",
    "fp = len(pos_pr) - len(true_positive)\n",
    "tn = 0\n",
    "fn = len(tot_tp_data) - len(pos_pr)\n",
    "\n",
    "precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "specificity = tn / (tn + fp) if tn + fp != 0 else 0\n",
    "f1_score_value = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "far = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "\n",
    "# Detection success rate: proportion of predictions made before actual failure\n",
    "detection_success_rate = len(pos_prd_er) / len(tot_tp_data)\n",
    "mean_prediction_advance_time = np.mean(pos_prd_er) if len(pos_prd_er) > 0 else 0\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Detection success rate: {detection_success_rate:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Mean prediction advance time (cycles): {mean_prediction_advance_time:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b88e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective warning count: 36\n",
      "Ineffective warning count: 8\n",
      "Failed warning count: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Add labels based on prediction performance\n",
    "labels = []\n",
    "for i in range(len(tot_tp_data)):\n",
    "    if prd_er[i] >= 0:\n",
    "        if tot_tp_pre[i] in true_positive:\n",
    "            labels.append(\"Effective warning\")\n",
    "        else:\n",
    "            labels.append(\"Ineffective warning\")\n",
    "    else:\n",
    "        labels.append(\"Failed warning\")\n",
    "\n",
    "# Create a DataFrame with prediction results\n",
    "df_results = pd.DataFrame({\n",
    "    'Warning time': tot_tp_pre,\n",
    "    'Knee time': tot_tp_data,\n",
    "    'Label': labels\n",
    "})\n",
    "\n",
    "# Count the number of each warning type\n",
    "effective_count = labels.count(\"Effective warning\")\n",
    "ineffective_count = labels.count(\"Ineffective warning\")\n",
    "failed_count = labels.count(\"Failed warning\")\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Effective warning count: {effective_count}\")\n",
    "print(f\"Ineffective warning count: {ineffective_count}\")\n",
    "print(f\"Failed warning count: {failed_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590cb10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
