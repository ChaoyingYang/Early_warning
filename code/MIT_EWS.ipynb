{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5294080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def bacon_watts_method(time, capacity):\n",
    "    \"\"\"\n",
    "    Detect the turning point (knee point) in the capacity curve using the Bacon-Watts method.\n",
    "    \n",
    "    Parameters:\n",
    "    - time: Original time data (e.g., cycle number)\n",
    "    - capacity: Original capacity data\n",
    "\n",
    "    Returns:\n",
    "    - best_split_index: Index of the knee point in the data\n",
    "    - jump_point_time: Original time value corresponding to the knee point\n",
    "    - jump_point_capacity: Original capacity value corresponding to the knee point\n",
    "    \"\"\"\n",
    "    n = len(time)\n",
    "    best_split_index = 0\n",
    "    min_total_error = float('inf')\n",
    "\n",
    "    # Traverse all possible split points\n",
    "    for split in range(1, n - 1):\n",
    "        # Split the data into left and right segments\n",
    "        time_left, capacity_left = time[:split], capacity[:split]\n",
    "        time_right, capacity_right = time[split:], capacity[split:]\n",
    "        \n",
    "        # Fit linear models to the left and right segments\n",
    "        model_left = LinearRegression().fit(time_left.reshape(-1, 1), capacity_left)\n",
    "        model_right = LinearRegression().fit(time_right.reshape(-1, 1), capacity_right)\n",
    "        \n",
    "        # Calculate prediction errors for both segments\n",
    "        predicted_left = model_left.predict(time_left.reshape(-1, 1))\n",
    "        predicted_right = model_right.predict(time_right.reshape(-1, 1))\n",
    "        error_left = np.sum((capacity_left - predicted_left) ** 2)\n",
    "        error_right = np.sum((capacity_right - predicted_right) ** 2)\n",
    "        \n",
    "        # Compute total error\n",
    "        total_error = error_left + error_right\n",
    "\n",
    "        # Update best split point if current split yields a smaller error\n",
    "        if total_error < min_total_error:\n",
    "            min_total_error = total_error\n",
    "            best_split_index = split\n",
    "\n",
    "    # Retrieve the original values at the knee point\n",
    "    jump_point_time = time[best_split_index]\n",
    "    jump_point_capacity = capacity[best_split_index]\n",
    "\n",
    "    return best_split_index, jump_point_time, jump_point_capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20011d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tool import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from common1 import *\n",
    "from net import CRNN\n",
    "import scipy \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_lbl = load_obj('./data/ne_data/label_train')\n",
    "ne_train_name = list(train_lbl.keys())\n",
    "\n",
    "valid_lbl = load_obj('./data/ne_data/label_test')\n",
    "ne_valid_name = list(valid_lbl.keys())\n",
    "\n",
    "test_lbl = load_obj('./data/ne_data/label_sec')\n",
    "ne_test_name = list(test_lbl.keys())\n",
    "\n",
    "all_loader = dict()\n",
    "for name in ne_train_name:\n",
    "    tmp_lbl = train_lbl.get(name)\n",
    "    all_loader.update({name: {'lbl': tmp_lbl}})\n",
    "    \n",
    "for name in ne_valid_name:\n",
    "    tmp_lbl = valid_lbl.get(name)\n",
    "    all_loader.update({name: {'lbl': tmp_lbl}})\n",
    "    \n",
    "for name in ne_test_name:\n",
    "    tmp_lbl = test_lbl.get(name)\n",
    "    all_loader.update({name: {'lbl': tmp_lbl}})\n",
    "\n",
    "num_train = len(ne_train_name)\n",
    "num_test = len(ne_test_name)\n",
    "num_valid = len(ne_valid_name)\n",
    "\n",
    "smoothing_factor = 0.5\n",
    "\n",
    "tot_name = ne_train_name + ne_valid_name + ne_test_name\n",
    "\n",
    "tot_cap, tot_eol, tot_tp = [], [], []\n",
    "for name in tot_name:\n",
    "    tmp_cap = all_loader[name]['lbl']\n",
    "\n",
    "    Cycle = np.array([x for x in range(len(tmp_cap))])\n",
    "    Capacity = np.array(tmp_cap)\n",
    "\n",
    "    # Outlier detection and interpolation\n",
    "    # Use the simple 3σ rule to detect outliers; you can adjust the detection method as needed\n",
    "    mean = np.mean(Capacity)\n",
    "    std_dev = np.std(Capacity)\n",
    "    outliers = (Capacity < mean - 3 * std_dev) | (Capacity > mean + 3 * std_dev)\n",
    "    \n",
    "    # Fill outliers using linear interpolation\n",
    "    Capacity[outliers] = np.interp(Cycle[outliers], Cycle[~outliers], Capacity[~outliers])\n",
    "\n",
    "    # Apply curve smoothing\n",
    "    spline = UnivariateSpline(Cycle, Capacity, s=smoothing_factor)\n",
    "    smoothed_capacity = spline(Cycle)\n",
    "    \n",
    "    tot_eol.append(len(tmp_cap))\n",
    "    tot_cap.append(np.array(smoothed_capacity))\n",
    "    \n",
    "    capacity = np.array(tot_cap[-1])  # Replace with your actual data\n",
    "    time = np.array([x for x in range(len(capacity))])\n",
    "\n",
    "    # Use Bacon-Watts method to detect the knee point\n",
    "    best_split_index, jump_point_time, jump_point_capacity = bacon_watts_method(time, capacity)\n",
    "    tot_tp.append(jump_point_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def func(x, D1, D2):\n",
    "    return D1 * np.sqrt(x) + D2 * np.power(x, 1.5)\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.where(y_true == 0, 1e-6, y_true)  # 避免除0\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# 结果容器\n",
    "tot_r2 = []\n",
    "tot_rmse = []\n",
    "tot_mape = []\n",
    "\n",
    "tot_jl = []\n",
    "tot_Tcap = []\n",
    "\n",
    "for num in range(len(tot_cap)):\n",
    "    # 生命周期区间\n",
    "    Cycle = np.arange(tot_eol[num])\n",
    "    \n",
    "    # 平滑容量 + 损失计算\n",
    "    cap_tmp = savgol_filter(tot_cap[num][:tot_eol[num]], 20, 4)\n",
    "    Qloss = tot_cap[num][0] - cap_tmp\n",
    "\n",
    "    # 拟合整个区间\n",
    "    popt, _ = curve_fit(func, Cycle, Qloss)\n",
    "    \n",
    "    # 预测全周期\n",
    "    Qloss_pred = func(Cycle, *popt)\n",
    "\n",
    "    # 存储拟合曲线和真实曲线\n",
    "    tot_jl.append(Qloss_pred)\n",
    "    tot_Tcap.append(Qloss)\n",
    "\n",
    "    # 全周期评估\n",
    "    tot_r2.append(r2_score(Qloss, Qloss_pred))\n",
    "    tot_rmse.append(rmse(Qloss, Qloss_pred))\n",
    "    tot_mape.append(mape(Qloss, Qloss_pred))\n",
    "\n",
    "# 平均指标输出\n",
    "print(\"平均 R²:\", np.mean(tot_r2))\n",
    "print(\"平均 RMSE:\", np.mean(tot_rmse))\n",
    "print(\"平均 MAPE (%):\", np.mean(tot_mape))\n",
    "\n",
    "# 可视化 R²\n",
    "x = np.arange(1, len(tot_cap) + 1)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, tot_r2, color='blue', label='R²')\n",
    "plt.xlabel('Sample', fontsize=14)\n",
    "plt.ylabel('R² Score', fontsize=14)\n",
    "plt.title('R² of full-length fit across samples', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6de297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection success rate: 0.9467\n",
      "Precision: 0.9718\n",
      "Mean prediction advance time (cycles): 116.9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define nonlinear model: y = A * sqrt(x) + B * x^1.5\n",
    "def model(A, B, x):\n",
    "    return A * np.sqrt(x) + B * np.power(x, 1.5)\n",
    "\n",
    "\n",
    "# Recursive Least Squares with regularization and soft constraints\n",
    "class RecursiveLeastSquares:\n",
    "    def __init__(self, lambda_factor=0.99, delta=1e3, regularization_factor=0.1):\n",
    "        # Initialize parameters\n",
    "        self.lambda_factor = lambda_factor  # Forgetting factor\n",
    "        self.regularization_factor = regularization_factor  # Regularization strength\n",
    "        self.theta = np.array([1e-1, 0], dtype=np.float64)  # Initial parameters [A, B]\n",
    "        self.P = np.eye(2, dtype=np.float64) * delta  # Initial covariance matrix\n",
    "    \n",
    "    def update(self, x, y):\n",
    "        # Feature vector φ_t = [sqrt(x), x^1.5]\n",
    "        phi_t = np.array([np.sqrt(x), np.power(x, 1.5)], dtype=np.float64)\n",
    "        \n",
    "        # Gain matrix K_t\n",
    "        P_phi = self.P @ phi_t\n",
    "        gain = P_phi / (self.lambda_factor + phi_t.T @ P_phi)\n",
    "        \n",
    "        # Update parameters θ_t\n",
    "        self.theta += gain * (y - phi_t.T @ self.theta)\n",
    "        \n",
    "        # Soft constraint on A to prevent it from becoming negative\n",
    "        if self.theta[0] < 0:\n",
    "            self.theta[0] += self.regularization_factor * abs(self.theta[0]) * 4\n",
    "            if self.theta[0] < 0:\n",
    "                self.theta[0] = 0\n",
    "        \n",
    "        # Update covariance matrix P_t\n",
    "        self.P = (self.P - np.outer(gain, phi_t.T @ self.P)) / self.lambda_factor\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.theta  # Return [A, B]\n",
    "\n",
    "\n",
    "# Batch initialization\n",
    "def batch_initialize(x_data, y_data, rls, batch_size):\n",
    "    for i in range(batch_size):\n",
    "        rls.update(x_data[i], y_data[i])\n",
    "\n",
    "\n",
    "# Global experiment parameters\n",
    "window_size = 30  # Rolling window size\n",
    "increase_length = 12\n",
    "threshold = 5\n",
    "\n",
    "\n",
    "# Split temporary dataset into validation and test sets\n",
    "valid_tot_cap, test_tot_cap, valid_tot_eol, test_tot_eol, valid_tot_tp, test_tot_tp = train_test_split(\n",
    "    tot_cap, tot_eol, tot_tp, test_size=0.6, random_state=42\n",
    ")\n",
    "\n",
    "tot_cap_data = test_tot_cap\n",
    "tot_tp_data = test_tot_tp\n",
    "tot_eol_data = test_tot_eol\n",
    "threshold = threshold * 1e-6\n",
    "tot_tp_pre = []\n",
    "\n",
    "# Run experiments on each sample\n",
    "for num in range(len(tot_cap_data)):\n",
    "    Cycle = [x for x in range(5000)]\n",
    "    Qloss = tot_cap_data[num][0] - np.array(tot_cap_data[num])\n",
    "    y_data = Qloss[:tot_eol_data[num]]\n",
    "    x_data = np.array(Cycle[:tot_eol_data[num]]) / 5000\n",
    "    cyc = Cycle[:tot_eol_data[num]]\n",
    "\n",
    "    # Initialize RLS\n",
    "    rls = RecursiveLeastSquares(lambda_factor=0.99, delta=1e3)\n",
    "\n",
    "    # Batch initialization\n",
    "    batch_initialize(x_data, y_data, rls, batch_size=10)\n",
    "\n",
    "    # Save history\n",
    "    A_history = []\n",
    "    B_history = []\n",
    "    r2_history = []\n",
    "    fitted_capacity_history = []\n",
    "\n",
    "    # Recursive update for each data point\n",
    "    for t, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "        rls.update(x, y)\n",
    "        A, B = rls.get_params()\n",
    "\n",
    "        A_history.append(A)\n",
    "        B_history.append(B)\n",
    "\n",
    "        y_fit_current = model(A, B, x_data[:t + 1])\n",
    "        r2_value = r2_score(y_data[:t + 1], y_fit_current)\n",
    "        r2_history.append(r2_value)\n",
    "\n",
    "        fitted_capacity_history.append(y_fit_current[-1])  # Save last fitted value\n",
    "\n",
    "    t = np.arange(0, len(A_history))  # Time series\n",
    "    series = pd.Series(A_history)\n",
    "\n",
    "    # Compute rolling variance\n",
    "    variance_signal = series.rolling(window=window_size).var()\n",
    "    moving_avg = np.convolve(variance_signal, np.ones(window_size) / window_size, mode='valid')\n",
    "    padding = np.full(window_size - 1, np.nan)  # Padding with NaN\n",
    "    variance_signal = np.concatenate((padding, moving_avg))\n",
    "\n",
    "    increasing_start_idx = None\n",
    "    increase_count = 0\n",
    "\n",
    "    # Detect start index of continuous increase\n",
    "    for idx in range(100, len(variance_signal)):\n",
    "        if variance_signal[idx] > variance_signal[idx - 1] and variance_signal[idx] > threshold:\n",
    "            increase_count += 1\n",
    "            if increase_count >= increase_length:\n",
    "                increasing_start_idx = idx - increase_length + 1\n",
    "                break\n",
    "        else:\n",
    "            increase_count = 0  # Reset counter\n",
    "\n",
    "    # Ensure no out-of-bound access\n",
    "    if increasing_start_idx is not None:\n",
    "        increasing_start_time = increasing_start_idx\n",
    "    else:\n",
    "        increasing_start_time = len(A_history)\n",
    "\n",
    "    tot_tp_pre.append(increasing_start_time)\n",
    "\n",
    "\n",
    "# Evaluate prediction results\n",
    "prd_er = np.array(tot_tp_data) - np.array(tot_tp_pre)\n",
    "pos_prd_er = []\n",
    "pos_tp = []\n",
    "pos_pr = []\n",
    "true_positive = []\n",
    "all_prediction_advance_times = []  # Store all advance times\n",
    "\n",
    "for i in range(len(prd_er)):\n",
    "    if tot_tp_pre[i] is not None:  # Skip None values\n",
    "        time_diff = prd_er[i]  # Prediction lead time\n",
    "        if time_diff >= 0:\n",
    "            pos_pr.append(tot_tp_pre[i])\n",
    "            pos_prd_er.append(time_diff)\n",
    "\n",
    "        # Count true positives: predicted within 20% of EOL\n",
    "        if 0 < time_diff <= tot_eol_data[i] * 0.2:\n",
    "            true_positive.append(tot_tp_pre[i])\n",
    "\n",
    "# Compute metrics\n",
    "tp = len(true_positive)\n",
    "fp = len(pos_pr) - len(true_positive)\n",
    "tn = 0\n",
    "fn = len(tot_tp_data) - len(pos_pr)\n",
    "\n",
    "precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "specificity = tn / (tn + fp) if tn + fp != 0 else 0\n",
    "f1_score_value = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "far = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "\n",
    "mean_prediction_advance_time = np.mean(pos_prd_er) if len(pos_prd_er) > 0 else 0\n",
    "detection_success_rate = len(pos_prd_er) / len(tot_tp_data)\n",
    "\n",
    "print(f\"Detection success rate: {detection_success_rate:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Mean prediction advance time (cycles): {mean_prediction_advance_time:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f9f38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective warning count: 69\n",
      "Ineffective warning count: 2\n",
      "Failed warning count: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Add labels based on prediction performance\n",
    "labels = []\n",
    "for i in range(len(tot_tp_data)):\n",
    "    if prd_er[i] >= 0:\n",
    "        if tot_tp_pre[i] in true_positive:\n",
    "            labels.append(\"Effective warning\")\n",
    "        else:\n",
    "            labels.append(\"Ineffective warning\")\n",
    "    else:\n",
    "        labels.append(\"Failed warning\")\n",
    "\n",
    "# Create a DataFrame with prediction results\n",
    "df_results = pd.DataFrame({\n",
    "    'Warning time': tot_tp_pre,\n",
    "    'Knee time': tot_tp_data,\n",
    "    'Label': labels\n",
    "})\n",
    "\n",
    "# Count the number of each warning type\n",
    "effective_count = labels.count(\"Effective warning\")\n",
    "ineffective_count = labels.count(\"Ineffective warning\")\n",
    "failed_count = labels.count(\"Failed warning\")\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Effective warning count: {effective_count}\")\n",
    "print(f\"Ineffective warning count: {ineffective_count}\")\n",
    "print(f\"Failed warning count: {failed_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
